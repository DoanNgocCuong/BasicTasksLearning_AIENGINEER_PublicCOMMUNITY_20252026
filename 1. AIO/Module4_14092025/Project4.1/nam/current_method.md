# Ref:
- https://aistudio.google.com/prompts/1Wx081g1dLhv2yotR0LhVG0D2Cji1t2pM

# Version 1
## **1. Má»¥c TiÃªu**

Má»¥c tiÃªu cá»§a Version 1.0 lÃ  xÃ¢y dá»±ng vÃ  Ä‘Ã¡nh giÃ¡ má»™t mÃ´ hÃ¬nh baseline Ä‘áº§u tiÃªn cho bÃ i toÃ¡n phÃ¢n loáº¡i chá»§ Ä‘á» bÃ i bÃ¡o khoa há»c trÃªn bá»™ dá»¯ liá»‡u ArXiv Ä‘Ã£ qua tiá»n xá»­ lÃ½. CÃ¡c má»¥c tiÃªu cá»¥ thá»ƒ bao gá»“m:
-   XÃ¡c thá»±c tÃ­nh hiá»‡u quáº£ cá»§a bá»™ dá»¯ liá»‡u `arxiv_perfectly_balanced.csv`.
-   Triá»ƒn khai kiáº¿n trÃºc phÃ¢n loáº¡i phÃ¢n cáº¥p hai táº§ng (Hierarchical Classification).
-   Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y cá»• Ä‘iá»ƒn (TF-IDF + LightGBM) Ä‘á»ƒ thiáº¿t láº­p má»™t ngÆ°á»¡ng hiá»‡u suáº¥t (baseline) cÃ³ thá»ƒ Ä‘o lÆ°á»ng Ä‘Æ°á»£c.
-   ÄÃ¡nh giÃ¡ chi tiáº¿t hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh á»Ÿ cáº£ hai táº§ng vÃ  phÃ¢n tÃ­ch cÃ¡c Ä‘iá»ƒm cáº§n cáº£i thiá»‡n.

## **2. Kiáº¿n TrÃºc & PhÆ°Æ¡ng PhÃ¡p Thá»±c Hiá»‡n**

Kiáº¿n trÃºc tá»•ng thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng theo má»™t pipeline gá»“m 3 giai Ä‘oáº¡n chÃ­nh: TrÃ­ch xuáº¥t Äáº·c trÆ°ng, Huáº¥n luyá»‡n MÃ´ hÃ¬nh PhÃ¢n cáº¥p, vÃ  Quy trÃ¬nh Dá»± Ä‘oÃ¡n.

### **2.1. TrÃ­ch Xuáº¥t Äáº·c TrÆ°ng (Feature Extraction)**

-   **PhÆ°Æ¡ng phÃ¡p:** Term Frequency-Inverse Document Frequency (TF-IDF).
-   **Chi tiáº¿t:**
    -   Sá»­ dá»¥ng `TfidfVectorizer` cá»§a Scikit-learn.
    -   Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng á»Ÿ `max_features=5000` tá»« phá»• biáº¿n nháº¥t Ä‘á»ƒ cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  tá»‘c Ä‘á»™ tÃ­nh toÃ¡n.
    -   Loáº¡i bá» cÃ¡c tá»« dá»«ng (stop words) tiáº¿ng Anh.
-   **Káº¿t quáº£:** Má»—i abstract Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vector thÆ°a (sparse vector) 5000 chiá»u.

### **2.2. Kiáº¿n TrÃºc MÃ´ HÃ¬nh PhÃ¢n Cáº¥p Hai Táº§ng**

Äá»ƒ xá»­ lÃ½ cáº¥u trÃºc cha-con cá»§a nhÃ£n, chÃºng tÃ´i Ä‘Ã£ triá»ƒn khai má»™t há»‡ thá»‘ng gá»“m hai táº§ng mÃ´ hÃ¬nh:

#### **Táº§ng 1: Parent Classifier (Bá»™ phÃ¢n loáº¡i NhÃ£n Cha)**
-   **Nhiá»‡m vá»¥:** Dá»± Ä‘oÃ¡n má»™t hoáº·c nhiá»u trong sá»‘ 17 nhÃ£n cha chÃ­nh (vd: `cs`, `math`, `hep`) tá»« abstract cá»§a bÃ i bÃ¡o.
-   **MÃ´ hÃ¬nh:** `OneVsRestClassifier` káº¿t há»£p vá»›i `LGBMClassifier`.
-   **Xá»­ lÃ½ Máº¥t cÃ¢n báº±ng:** Tham sá»‘ `class_weight='balanced'` Ä‘Æ°á»£c kÃ­ch hoáº¡t trong `LGBMClassifier`. ÄÃ¢y lÃ  má»™t bÆ°á»›c cá»±c ká»³ quan trá»ng, giÃºp thuáº­t toÃ¡n tá»± Ä‘á»™ng tÄƒng trá»ng sá»‘ cho cÃ¡c lá»›p cha thiá»ƒu sá»‘ (`econ`, `cond`), buá»™c mÃ´ hÃ¬nh pháº£i há»c chÃºng má»™t cÃ¡ch cÃ´ng báº±ng.

#### **Táº§ng 2: Child Classifiers (CÃ¡c bá»™ phÃ¢n loáº¡i NhÃ£n Con)**
-   **Nhiá»‡m vá»¥:** Vá»›i má»—i nhÃ£n cha Ä‘Æ°á»£c dá»± Ä‘oÃ¡n á»Ÿ Táº§ng 1, má»™t mÃ´ hÃ¬nh chuyÃªn biá»‡t á»Ÿ Táº§ng 2 sáº½ Ä‘Æ°á»£c kÃ­ch hoáº¡t Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n con cá»¥ thá»ƒ thuá»™c nhÃ£n cha Ä‘Ã³.
-   **Kiáº¿n trÃºc:** Má»™t táº­p há»£p gá»“m **15 mÃ´ hÃ¬nh con**, má»—i mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng vá»›i má»™t nhÃ£n cha cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n.
    -   *VÃ­ dá»¥:* Náº¿u Táº§ng 1 dá»± Ä‘oÃ¡n lÃ  `cs`, mÃ´ hÃ¬nh `cs_classifier` cá»§a Táº§ng 2 sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n con nhÆ° `cs.AI`, `cs.CV`, `cs.LG`,...
-   **MÃ´ hÃ¬nh:** Má»—i bá»™ phÃ¢n loáº¡i con cÅ©ng lÃ  má»™t `OneVsRestClassifier` vá»›i `LGBMClassifier`, cÅ©ng sá»­ dá»¥ng `class_weight='balanced'`.

### **2.3. Quy TrÃ¬nh Huáº¥n Luyá»‡n & Dá»± ÄoÃ¡n**

1.  **Huáº¥n luyá»‡n:**
    -   Huáº¥n luyá»‡n mÃ´ hÃ¬nh Táº§ng 1 trÃªn toÃ n bá»™ táº­p train (23,995 máº«u) vá»›i 17 nhÃ£n cha.
    -   Vá»›i má»—i nhÃ£n cha, lá»c ra cÃ¡c máº«u trong táº­p train thuá»™c vá» nhÃ£n cha Ä‘Ã³ vÃ  huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh Táº§ng 2 tÆ°Æ¡ng á»©ng.
2.  **Dá»± Ä‘oÃ¡n (trÃªn táº­p test):**
    -   **BÆ°á»›c 1:** ÄÆ°a abstract vÃ o mÃ´ hÃ¬nh Táº§ng 1 Ä‘á»ƒ nháº­n vá» cÃ¡c nhÃ£n cha dá»± Ä‘oÃ¡n (vÃ­ dá»¥: `['cs', 'math']`).
    -   **BÆ°á»›c 2:** Vá»›i má»—i nhÃ£n cha dá»± Ä‘oÃ¡n Ä‘Æ°á»£c, kÃ­ch hoáº¡t mÃ´ hÃ¬nh Táº§ng 2 tÆ°Æ¡ng á»©ng.
        -   `cs_classifier` sáº½ dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n con cá»§a `cs`.
        -   `math_classifier` sáº½ dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n con cá»§a `math`.
    -   **BÆ°á»›c 3:** Gá»™p táº¥t cáº£ cÃ¡c nhÃ£n con dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tá»« cÃ¡c mÃ´ hÃ¬nh Táº§ng 2 Ä‘á»ƒ ra káº¿t quáº£ cuá»‘i cÃ¹ng.

## **3. Káº¿t Quáº£ Thá»­ Nghiá»‡m (Version 1.0)**

| Táº§ng ÄÃ¡nh GiÃ¡ | Metric | GiÃ¡ Trá»‹ | Ghi ChÃº |
| :--- | :--- | :--- | :--- |
| **Táº§ng 1 (NhÃ£n Cha)** | **F1-Score (Weighted Avg)** | **0.6483** | **Metric chÃ­nh**, pháº£n Ã¡nh hiá»‡u suáº¥t tá»•ng thá»ƒ cÃ³ trá»ng sá»‘. |
| | F1-Score (Macro Avg) | 0.6474 | Cho tháº¥y mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t trÃªn cáº£ lá»›p Ä‘a sá»‘ vÃ  thiá»ƒu sá»‘. |
| | F1-Score (Samples Avg) | 0.6396 | Hiá»‡u suáº¥t trung bÃ¬nh trÃªn tá»«ng máº«u, há»¯u Ã­ch cho Ä‘a nhÃ£n. |
| **ToÃ n Há»‡ Thá»‘ng (NhÃ£n Con)** | **F1-Score (Weighted Avg)** | **0.4047** | Pháº£n Ã¡nh hiá»‡u suáº¥t dá»± Ä‘oÃ¡n nhÃ£n con cuá»‘i cÃ¹ng. |
| | F1-Score (Samples Avg) | 0.4142 | |
| | F1-Score (Macro Avg) | 0.2543 | **Ráº¥t tháº¥p**, cho tháº¥y mÃ´ hÃ¬nh cá»±c ká»³ khÃ³ khÄƒn vá»›i cÃ¡c lá»›p con hiáº¿m. |

## **4. PhÃ¢n TÃ­ch & ÄÃ¡nh GiÃ¡**

### **4.1. Äiá»ƒm TÃ­ch Cá»±c**

-   **Chiáº¿n lÆ°á»£c dá»¯ liá»‡u Ä‘Æ°á»£c xÃ¡c thá»±c:** Viá»‡c F1-macro vÃ  F1-weighted á»Ÿ Táº§ng 1 gáº§n nhÆ° báº±ng nhau (chÃªnh lá»‡ch chá»‰ 0.0009) kháº³ng Ä‘á»‹nh ráº±ng chiáº¿n lÆ°á»£c **cÃ¢n báº±ng Ä‘Æ¡n/Ä‘a nhÃ£n** káº¿t há»£p vá»›i `class_weight='balanced'` lÃ  hoÃ n toÃ n Ä‘Ãºng Ä‘áº¯n. MÃ´ hÃ¬nh khÃ´ng bá»‹ thiÃªn vá»‹ náº·ng vá» cÃ¡c lá»›p cha Ä‘a sá»‘.
-   **Thiáº¿t láº­p Baseline thÃ nh cÃ´ng:** MÃ´ hÃ¬nh Ä‘Ã£ cung cáº¥p má»™t ngÆ°á»¡ng hiá»‡u suáº¥t rÃµ rÃ ng (F1 ~0.65 cho nhÃ£n cha, ~0.40 cho nhÃ£n con) Ä‘á»ƒ cÃ¡c phiÃªn báº£n tÆ°Æ¡ng lai cÃ³ thá»ƒ so sÃ¡nh vÃ  cáº£i thiá»‡n.

### **4.2. Háº¡n Cháº¿ & NguyÃªn NhÃ¢n Hiá»‡u Suáº¥t**

Káº¿t quáº£ hiá»‡n táº¡i lÃ  má»™t baseline tá»‘t, nhÆ°ng chÆ°a cao. NguyÃªn nhÃ¢n khÃ´ng náº±m á»Ÿ khÃ¢u chuáº©n bá»‹ dá»¯ liá»‡u mÃ  Ä‘áº¿n tá»« cÃ¡c yáº¿u tá»‘ sau:

1.  **Sá»¥t giáº£m hiá»‡u suáº¥t tá»« Táº§ng 1 -> Táº§ng 2:** F1-weighted giáº£m tá»« **0.65 xuá»‘ng 0.40**. Äáº·c biá»‡t, F1-macro giáº£m máº¡nh tá»« **0.65 xuá»‘ng 0.25**, cho tháº¥y nÃºt tháº¯t cá»• chai náº±m á»Ÿ viá»‡c dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n con. ÄÃ¢y lÃ  bÃ i toÃ¡n phÃ¢n loáº¡i chi tiáº¿t (fine-grained) vá»›i hÃ ng trÄƒm lá»›p con, trong Ä‘Ã³ ráº¥t nhiá»u lá»›p cÃ³ sá»‘ lÆ°á»£ng máº«u cá»±c ká»³ Ã­t (váº¥n Ä‘á» Ä‘uÃ´i dÃ i - long-tail problem), khiáº¿n mÃ´ hÃ¬nh khÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ há»c.

2.  **Äiá»ƒm mÃ¹ ngá»¯ nghÄ©a cá»§a TF-IDF:** `TF-IDF` chá»‰ Ä‘áº¿m tá»«, khÃ´ng hiá»ƒu nghÄ©a. NÃ³ khÃ´ng nháº­n ra ráº±ng "machine learning" vÃ  "deep learning" cÃ³ liÃªn quan Ä‘áº¿n nhau. ÄÃ¢y lÃ  háº¡n cháº¿ lá»›n nháº¥t vá» máº·t trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng, ngÄƒn mÃ´ hÃ¬nh "hiá»ƒu" sÃ¢u hÆ¡n vá» ná»™i dung abstract.

3.  **MÃ´ hÃ¬nh Baseline chÆ°a Ä‘Æ°á»£c tinh chá»‰nh:** CÃ¡c tham sá»‘ cá»§a LightGBM (`n_estimators=100`) vÃ  TF-IDF (`max_features=5000`) Ä‘ang á»Ÿ má»©c cÆ¡ báº£n Ä‘á»ƒ cháº¡y nhanh. ChÃºng chÆ°a Ä‘Æ°á»£c tá»‘i Æ°u Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t cao nháº¥t.

## **5. HÆ°á»›ng Cáº£i Thiá»‡n cho Version 2.0**

Ná»n táº£ng dá»¯ liá»‡u Ä‘Ã£ vá»¯ng cháº¯c. Lá»™ trÃ¬nh cáº£i thiá»‡n cho phiÃªn báº£n tiáº¿p theo sáº½ táº­p trung vÃ o viá»‡c nÃ¢ng cáº¥p mÃ´ hÃ¬nh.

-   **Æ¯u tiÃªn #1 (TÃ¡c Ä‘á»™ng lá»›n nháº¥t): NÃ¢ng cáº¥p Feature Extraction.**
    -   **Thá»­ nghiá»‡m:** Thay tháº¿ TF-IDF báº±ng cÃ¡c mÃ´ hÃ¬nh nhÃºng tá»« cÃ³ kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a nhÆ° **SciBERT**. ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Transformer Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn má»™t kho vÄƒn báº£n khoa há»c khá»•ng lá»“, há»©a háº¹n sáº½ mang láº¡i sá»± cáº£i thiá»‡n Ä‘á»™t phÃ¡.

-   **Æ¯u tiÃªn #2: Tinh chá»‰nh siÃªu tham sá»‘ (Hyperparameter Tuning).**
    -   **Thá»­ nghiá»‡m:** Sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n nhÆ° Optuna hoáº·c Hyperopt Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m ra bá»™ tham sá»‘ tá»‘t nháº¥t cho `LGBMClassifier` (vÃ­ dá»¥: `n_estimators`, `learning_rate`, `num_leaves`, ...).

-   **Æ¯u tiÃªn #3 (TÃ¹y chá»n): Tá»‘i Æ°u hÃ³a ngÆ°á»¡ng quyáº¿t Ä‘á»‹nh.**
    -   **Thá»­ nghiá»‡m:** Sau khi cÃ³ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t, tÃ¬m má»™t ngÆ°á»¡ng quyáº¿t Ä‘á»‹nh (threshold) tá»‘i Æ°u cho má»—i nhÃ£n thay vÃ¬ dÃ¹ng máº·c Ä‘á»‹nh 0.5 Ä‘á»ƒ tá»‘i Ä‘a hÃ³a F1-score.

## **6. Káº¿t Luáº­n Chung**

Version 1.0 Ä‘Ã£ thÃ nh cÃ´ng trong viá»‡c xÃ¢y dá»±ng má»™t pipeline hoÃ n chá»‰nh vÃ  thiáº¿t láº­p má»™t baseline hiá»‡u suáº¥t Ä‘Ã¡ng tin cáº­y. PhÃ¢n tÃ­ch Ä‘Ã£ chá»‰ ra ráº±ng chiáº¿n lÆ°á»£c chuáº©n bá»‹ dá»¯ liá»‡u lÃ  Ä‘Ãºng Ä‘áº¯n vÃ  cÃ¡c Ä‘iá»ƒm ngháº½n vá» hiá»‡u suáº¥t náº±m á»Ÿ kháº£ nÄƒng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÃ  sá»± tinh chá»‰nh cá»§a mÃ´ hÃ¬nh. CÃ¡c bÆ°á»›c tiáº¿p theo sáº½ táº­p trung vÃ o viá»‡c giáº£i quyáº¿t cÃ¡c Ä‘iá»ƒm ngháº½n nÃ y.

# Version 2
## Script:
```python
# ===================================================================
#                      VERSION 2.2: SPACY + FASTTEXT + OPTUNA
# ===================================================================
# Script nÃ y thay tháº¿ hoÃ n toÃ n NLTK báº±ng spaCy Ä‘á»ƒ giáº£i quyáº¿t triá»‡t Ä‘á»ƒ
# lá»—i LookupError, trong khi váº«n giá»¯ nguyÃªn cÃ¡c cáº£i tiáº¿n vá» ngá»¯ nghÄ©a
# (FastText) vÃ  tá»‘i Æ°u hÃ³a (Optuna).
# ===================================================================

# ### BÆ¯á»šC 0: CÃ€I Äáº¶T Cáº¦N THIáº¾T ###
# Cháº¡y Ã´ nÃ y TRÆ¯á»šC TIÃŠN trong Colab Ä‘á»ƒ cÃ i Ä‘áº·t spaCy vÃ  táº£i mÃ´ hÃ¬nh.
# !python -m spacy download en_core_web_sm

# ===================================================================
# PHáº¦N 0: CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T
# ===================================================================
# !pip install optuna
# !pip install gensim
# !python -m spacy download en_core_web_sm

print("ğŸš€ Äang import cÃ¡c thÆ° viá»‡n...")
import pandas as pd
import numpy as np
import ast
import pickle
import os
import json
from collections import Counter
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import f1_score, classification_report
from tqdm.auto import tqdm
import optuna
import gensim.downloader
import spacy # ### V2.2 Cáº¢I TIáº¾N: Thay tháº¿ NLTK báº±ng spaCy
import warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)
print("âœ… Import thÆ° viá»‡n hoÃ n táº¥t.")

# ===================================================================
# PHáº¦N 0B: Cáº¤U HÃŒNH
# ===================================================================
N_TARGET_LABELS = 17
OPTUNA_N_TRIALS = 25
OPTUNA_TIMEOUT = 3600

LGBM_FIXED_PARAMS = {
    'device': 'gpu',
    'random_state': 42,
    'n_jobs': -1,
    'class_weight': 'balanced'
}
print("âš¡ï¸ ÄÃ£ kÃ­ch hoáº¡t cháº¿ Ä‘á»™ huáº¥n luyá»‡n GPU vÃ  cáº¥u hÃ¬nh cho Version 2.2 (spaCy + FastText)!")

# ===================================================================
# PHáº¦N 1: Táº¢I Dá»® LIá»†U VÃ€ MÃ” HÃŒNH NLP
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 1/7] Táº£i dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh spaCy...")
# Táº£i mÃ´ hÃ¬nh spaCy nhá» gá»n, chá»‰ cáº§n tokenizer vÃ  stopwords
print("   - Äang táº£i mÃ´ hÃ¬nh spaCy 'en_core_web_sm'...")
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
stop_words = nlp.Defaults.stop_words
print("   - Táº£i spaCy hoÃ n táº¥t.")

FILE_PATH = "/content/drive/MyDrive/data/arxiv_perfectly_balanced.csv"
try:
    df = pd.read_csv(FILE_PATH)
    print(f"âœ… Táº£i thÃ nh cÃ´ng file: '{FILE_PATH}' ({len(df):,} máº«u)")
except FileNotFoundError:
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{FILE_PATH}'. HÃ£y Ä‘áº£m báº£o báº¡n Ä‘Ã£ káº¿t ná»‘i Google Drive vÃ  Ä‘Æ°á»ng dáº«n lÃ  chÃ­nh xÃ¡c.")
    exit()

df['parent_labels'] = df['parent_labels'].apply(ast.literal_eval)
df['child_labels'] = df['child_labels'].apply(ast.literal_eval)

# ===================================================================
# PHáº¦N 2: MÃƒ HÃ“A VÄ‚N Báº¢N Vá»šI FASTTEXT
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 2/7] Táº£i mÃ´ hÃ¬nh FastText vÃ  mÃ£ hÃ³a vÄƒn báº£n...")
print("   - Äang táº£i mÃ´ hÃ¬nh fasttext-wiki-news-subwords-300... (Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt)")
ft_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')
embedding_dim = ft_model.vector_size
print(f"âœ… Táº£i mÃ´ hÃ¬nh FastText thÃ nh cÃ´ng (sá»‘ chiá»u vector: {embedding_dim}).")

# ### V2.2 Cáº¢I TIáº¾N: HÃ m tiá»n xá»­ lÃ½ dÃ¹ng spaCy ###
def preprocess_text_spacy(text):
    doc = nlp(str(text).lower())
    tokens = [
        token.lemma_ for token in doc 
        if not token.is_stop and not token.is_punct and token.is_alpha
    ]
    return tokens

def abstract_to_vector(abstract, model, dim):
    tokens = preprocess_text_spacy(abstract)
    word_vectors = [model[word] for word in tokens if word in model.key_to_index]
    
    if not word_vectors:
        return np.zeros(dim)
    
    return np.mean(word_vectors, axis=0)

print("   - Äang táº¡o vector Ä‘áº·c trÆ°ng cho cÃ¡c abstract (sá»­ dá»¥ng spaCy)...")
tqdm.pandas(desc="MÃ£ hÃ³a Abstract")
df['abstract_vector'] = df['abstract'].progress_apply(lambda x: abstract_to_vector(x, ft_model, embedding_dim))

all_embeddings = np.vstack(df['abstract_vector'].values)
print(f"âœ… MÃ£ hÃ³a FastText hoÃ n táº¥t. KÃ­ch thÆ°á»›c ma tráº­n Ä‘áº·c trÆ°ng: {all_embeddings.shape}")

# ===================================================================
# PHáº¦N 3: TÃŒM SIÃŠU THAM Sá» Tá»I Æ¯U CHO Táº¦NG 1 Vá»šI OPTUNA
# ===================================================================
print(f"\nğŸš€ [BÆ°á»›c 3/7] Tá»‘i Æ°u siÃªu tham sá»‘ cho Táº§ng 1 vá»›i Optuna...")

parent_label_counts = Counter([item for sublist in df['parent_labels'] for item in sublist])
target_parents = [label for label, count in parent_label_counts.most_common(N_TARGET_LABELS)]
mlb_parent = MultiLabelBinarizer(classes=target_parents)
y_parent_binarized = mlb_parent.fit_transform(df['parent_labels'])
indices = df.index.values

X_train_emb, X_test_emb, y_train_p, y_test_p, indices_train, indices_test = train_test_split(
    all_embeddings, y_parent_binarized, indices, test_size=0.2, random_state=42
)

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 200, 1200, step=50),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'max_depth': trial.suggest_int('max_depth', 5, 25),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
    }
    params.update(LGBM_FIXED_PARAMS)
    
    X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(X_train_emb, y_train_p, test_size=0.25, random_state=42)
    
    model = OneVsRestClassifier(LGBMClassifier(**params), n_jobs=1)
    model.fit(X_train_opt, y_train_opt)
    
    preds = model.predict(X_val_opt)
    score = f1_score(y_val_opt, preds, average='weighted', zero_division=0)
    
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=OPTUNA_N_TRIALS, timeout=OPTUNA_TIMEOUT)
best_params_tier1 = study.best_params
print(f"âœ… Tá»‘i Æ°u hÃ³a hoÃ n táº¥t sau {len(study.trials)} trials.")
print(f"   - F1-Score tá»‘t nháº¥t trÃªn táº­p validation: {study.best_value:.4f}")
print(f"   - SiÃªu tham sá»‘ tá»‘i Æ°u: {best_params_tier1}")

# ===================================================================
# PHáº¦N 4: HUáº¤N LUYá»†N Táº¦NG 1 Vá»šI THAM Sá» Tá»T NHáº¤T
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 4/7] Huáº¥n luyá»‡n Táº§ng 1 trÃªn toÃ n bá»™ táº­p train vá»›i tham sá»‘ tá»‘t nháº¥t...")
final_params = best_params_tier1.copy()
final_params.update(LGBM_FIXED_PARAMS)
parent_model = OneVsRestClassifier(LGBMClassifier(**final_params), n_jobs=1)
parent_model.fit(X_train_emb, y_train_p)
print("âœ… Huáº¥n luyá»‡n mÃ´ hÃ¬nh Táº§ng 1 cuá»‘i cÃ¹ng hoÃ n táº¥t.")

# ===================================================================
# PHáº¦N 5: HUáº¤N LUYá»†N Táº¦NG 2
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 5/7] Huáº¥n luyá»‡n Táº§ng 2...")
tier2_classifiers, tier2_mlbs = {}, {}
df_train = df.loc[indices_train]
emb_train = all_embeddings[indices_train]
for parent_label in tqdm(mlb_parent.classes_, desc="Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Táº§ng 2"):
    indices_with_parent_local = [i for i, labels in enumerate(df_train['parent_labels']) if parent_label in labels]
    if len(indices_with_parent_local) < 20: continue
    df_child, X_child_emb = df_train.iloc[indices_with_parent_local], emb_train[indices_with_parent_local]
    y_child_raw = df_child['child_labels'].apply(lambda l: [c for c in l if c.startswith(parent_label)])
    if y_child_raw.apply(len).sum() == 0: continue
    mlb_child = MultiLabelBinarizer()
    y_child_binarized = mlb_child.fit_transform(y_child_raw)
    if y_child_binarized.shape[1] < 2: continue
    child_model = OneVsRestClassifier(LGBMClassifier(**final_params), n_jobs=1)
    child_model.fit(X_child_emb, y_child_binarized)
    tier2_classifiers[parent_label], tier2_mlbs[parent_label] = child_model, mlb_child
print(f"\nâœ… ÄÃ£ huáº¥n luyá»‡n {len(tier2_classifiers)} mÃ´ hÃ¬nh Táº§ng 2.")

# ===================================================================
# PHáº¦N 6: ÄÃNH GIÃ VÃ€ Táº O BÃO CÃO METRICS CHI TIáº¾T
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 6/7] ÄÃ¡nh giÃ¡ vÃ  táº¡o bÃ¡o cÃ¡o metrics chi tiáº¿t...")
df_test = df.loc[indices_test]
emb_test = all_embeddings[indices_test]

true_child_labels_raw = df_test['child_labels'].tolist()
mlb_all_children = MultiLabelBinarizer().fit(df['child_labels'])
y_test_child_true_binarized = mlb_all_children.transform(true_child_labels_raw)
y_pred_parent_binarized = parent_model.predict(emb_test)
final_parents_raw = mlb_parent.inverse_transform(y_pred_parent_binarized)
final_predictions_raw = []
for i in tqdm(range(len(df_test)), desc="Dá»± Ä‘oÃ¡n Táº§ng 2 trÃªn táº­p test"):
    predicted_parents = final_parents_raw[i]
    child_preds = set()
    if predicted_parents:
        emb_vector = emb_test[i:i+1]
        for parent in predicted_parents:
            if parent in tier2_classifiers:
                child_model, child_mlb = tier2_classifiers[parent], tier2_mlbs[parent]
                pred_child_binarized = child_model.predict(emb_vector)
                child_preds.update(child_mlb.inverse_transform(pred_child_binarized)[0])
    final_predictions_raw.append(sorted(list(child_preds)))
y_pred_child_final_binarized = mlb_all_children.transform(final_predictions_raw)

metrics_report = {}
report_parent_dict = classification_report(y_test_p, y_pred_parent_binarized, target_names=mlb_parent.classes_, output_dict=True, zero_division=0)
metrics_report['f1_macro_parent'] = report_parent_dict['macro avg']['f1-score']
metrics_report['f1_weighted_parent'] = report_parent_dict['weighted avg']['f1-score']
metrics_report['f1_samples_parent'] = f1_score(y_test_p, y_pred_parent_binarized, average='samples', zero_division=0)
metrics_report['f1_macro_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='macro', zero_division=0)
metrics_report['f1_weighted_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='weighted', zero_division=0)
metrics_report['f1_samples_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='samples', zero_division=0)
metrics_report['best_hyperparameters_tier1'] = study.best_params

print("\n" + "="*80)
print(" " * 16 + "BÃO CÃO HIá»†U SUáº¤T Há»† THá»NG - VERSION 2.2 (spaCy + FastText)")
print("="*80)
print(f"\n   - SIÃŠU THAM Sá» Tá»I Æ¯U (tá»« Optuna):")
for key, value in metrics_report['best_hyperparameters_tier1'].items():
    if isinstance(value, float):
        print(f"     - {key}: {value:.4f}")
    else:
        print(f"     - {key}: {value}")

print("\n--- Táº§ng 1 (Dá»± Ä‘oÃ¡n 17 NhÃ£n Cha chÃ­nh) ---")
print(f"   - â­ï¸ F1-Score (Weighted Avg): {metrics_report['f1_weighted_parent']:.4f}")
print(f"   - F1-Score (Macro Avg):        {metrics_report['f1_macro_parent']:.4f}")

print("\n--- ToÃ n Há»‡ Thá»‘ng (Dá»± Ä‘oÃ¡n NhÃ£n Con Cuá»‘i CÃ¹ng) ---")
print(f"   - â­ï¸ F1-Score (Weighted Avg): {metrics_report['f1_weighted_children_overall']:.4f}")
print(f"   - F1-Score (Macro Avg):        {metrics_report['f1_macro_children_overall']:.4f}")
print("\n" + "="*80)

# ===================================================================
# PHáº¦N 7: LÆ¯U Káº¾T QUáº¢ VÃ€ CÃC THÃ€NH PHáº¦N
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 7/7] LÆ°u káº¿t quáº£ vÃ  cÃ¡c thÃ nh pháº§n...")
MODEL_DIR = "/content/drive/MyDrive/data/saved_models_v2.2_spacy_fasttext_optuna/"
os.makedirs(MODEL_DIR, exist_ok=True)
with open(os.path.join(MODEL_DIR, 'tier1_classifier.pkl'), 'wb') as f: pickle.dump(parent_model, f)
with open(os.path.join(MODEL_DIR, 'tier2_classifiers.pkl'), 'wb') as f: pickle.dump(tier2_classifiers, f)
with open(os.path.join(MODEL_DIR, 'tier1_mlb.pkl'), 'wb') as f: pickle.dump(mlb_parent, f)
with open(os.path.join(MODEL_DIR, 'tier2_mlbs.pkl'), 'wb') as f: pickle.dump(tier2_mlbs, f)
with open(os.path.join(MODEL_DIR, 'metrics_report.json'), 'w') as f: json.dump(metrics_report, f, indent=4)
print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng cÃ¡c thÃ nh pháº§n mÃ´ hÃ¬nh vÃ o: {MODEL_DIR}")
```
**Version:** 2.2 - spaCy + FastText + Optuna  
**So vá»›i Version 1.0:** Thay tháº¿ TF-IDF báº±ng FastText embeddings vÃ  tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ báº±ng Optuna.

## **1. Má»¥c TiÃªu**

Version 2.2 Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i hai má»¥c tiÃªu chÃ­nh:
1.  **Giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u nÄƒng:** Thay tháº¿ pipeline TF-IDF báº±ng má»™t giáº£i phÃ¡p nháº¹ hÆ¡n (FastText) Ä‘á»ƒ cháº¡y mÆ°á»£t mÃ  trÃªn Colab.
2.  **Cáº£i thiá»‡n hiá»‡u suáº¥t:** Ká»³ vá»ng ráº±ng viá»‡c sá»­ dá»¥ng word embeddings cÃ³ ngá»¯ nghÄ©a vÃ  tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ sáº½ cho káº¿t quáº£ tá»‘t hÆ¡n V1.0.

## **2. Káº¿t Quáº£ Thá»­ Nghiá»‡m (Version 2.2)**

### **Báº£ng So SÃ¡nh Hiá»‡u Suáº¥t: V1.0 vs V2.2**

| Metric | V1.0 (TF-IDF 5k) | **V2.2 (FastText 300d)** | Thay Äá»•i | PhÃ¢n TÃ­ch Nhanh |
| :--- | :--- | :--- | :--- | :--- |
| **Thá»i gian Dá»± Ä‘oÃ¡n Táº§ng 2** | ~33 phÃºt | **~5 phÃºt** | **â†“ 85%** | âœ… **ThÃ nh cÃ´ng lá»›n** |
| **F1-Weighted (Táº§ng 1)** | 0.6483 | **0.6386** | **â†“ 1.5%** | âš ï¸ Giáº£m nháº¹ |
| **F1-Macro (Táº§ng 1)** | 0.6474 | **0.6359** | **â†“ 1.8%** | âš ï¸ Giáº£m nháº¹ |
| **F1-Weighted (Táº§ng 2)** | 0.4047 | **0.3732** | **â†“ 7.8%** | âš ï¸ Giáº£m Ä‘Ã¡ng ká»ƒ |
| **F1-Macro (Táº§ng 2)** | 0.2543 | **0.2319** | **â†“ 8.8%** | âš ï¸ Giáº£m Ä‘Ã¡ng ká»ƒ |

### **Hyperparameters Tá»‘i Æ¯u (tá»« Optuna):**
-   `n_estimators`: 550
-   `learning_rate`: 0.0522
-   `num_leaves`: 146
-   `max_depth`: **5**
-   `reg_alpha`: 0.0081
-   `reg_lambda`: 0.0563

## **3. PhÃ¢n TÃ­ch & ÄÃ¡nh GiÃ¡**

Version 2.2 lÃ  má»™t thá»­ nghiá»‡m cá»±c ká»³ thÃ nh cÃ´ng trong viá»‡c cung cáº¥p thÃ´ng tin, dÃ¹ cÃ¡c chá»‰ sá»‘ F1-score Ä‘Ã£ giáº£m.

### **3.1. Äiá»ƒm TÃ­ch Cá»±c**
-   **Váº¥n Ä‘á» Hiá»‡u nÄƒng Ä‘Ã£ Ä‘Æ°á»£c giáº£i quyáº¿t triá»‡t Ä‘á»ƒ:** Thá»i gian dá»± Ä‘oÃ¡n giáº£m tá»« 33 phÃºt xuá»‘ng chá»‰ cÃ²n 5 phÃºt lÃ  má»™t tháº¯ng lá»£i lá»›n, chá»©ng tá» FastText lÃ  má»™t lá»±a chá»n tuyá»‡t vá»i vá» máº·t tá»‘c Ä‘á»™ vÃ  tÃ i nguyÃªn. Pipeline hiá»‡n táº¡i Ä‘Ã£ sáºµn sÃ ng cho viá»‡c thá»­ nghiá»‡m nhanh chÃ³ng hÆ¡n.

### **3.2. PhÃ¢n TÃ­ch Sá»¥t Giáº£m Hiá»‡u Suáº¥t: Táº¡i Sao Káº¿t Quáº£ Láº¡i Tháº¥p HÆ¡n?**

ÄÃ¢y lÃ  Ä‘iá»ƒm máº¥u chá»‘t. DÃ¹ sá»­ dá»¥ng ká»¹ thuáº­t cÃ³ váº» "hiá»‡n Ä‘áº¡i" hÆ¡n, hiá»‡u suáº¥t láº¡i giáº£m. NguyÃªn nhÃ¢n Ä‘áº¿n tá»« hai yáº¿u tá»‘ chÃ­nh:

#### **1. Sá»± "Pha LoÃ£ng" TÃ­n Hiá»‡u cá»§a Vector Trung BÃ¬nh (Quan trá»ng nháº¥t)**
-   **TF-IDF (V1.0):** Ráº¥t giá»i trong viá»‡c nháº­n diá»‡n cÃ¡c **tá»« khÃ³a quan trá»ng nhÆ°ng hiáº¿m**. VÃ­ dá»¥, má»™t thuáº­t ngá»¯ nhÆ° "abelian variety" hoáº·c "hadronization" cÃ³ thá»ƒ cÃ³ Ä‘iá»ƒm TF-IDF ráº¥t cao vÃ  trá»Ÿ thÃ nh má»™t tÃ­n hiá»‡u cá»±c máº¡nh cho mÃ´ hÃ¬nh.
-   **FastText (V2.2):** PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng ta lÃ  **láº¥y trung bÃ¬nh vector cá»§a Táº¤T Cáº¢ cÃ¡c tá»«** trong abstract. Äiá»u nÃ y cÃ³ má»™t nhÆ°á»£c Ä‘iá»ƒm chÃ­ máº¡ng: vector cá»§a má»™t tá»« khÃ³a cá»±c ká»³ quan trá»ng nhÆ° "abelian variety" sáº½ bá»‹ "pha loÃ£ng" bá»Ÿi hÃ ng trÄƒm vector cá»§a cÃ¡c tá»« phá»• biáº¿n khÃ¡c nhÆ° "study", "result", "paper", "method",... TÃ­n hiá»‡u Ä‘áº·c trÆ°ng máº¡nh máº½ cá»§a tá»« khÃ³a Ä‘Ã³ bá»‹ máº¥t Ä‘i trong giÃ¡ trá»‹ trung bÃ¬nh.
-   **Káº¿t luáº­n:** Äá»‘i vá»›i vÄƒn báº£n khoa há»c, nÆ¡i cÃ¡c thuáº­t ngá»¯ cá»¥ thá»ƒ mang tÃ­nh quyáº¿t Ä‘á»‹nh, phÆ°Æ¡ng phÃ¡p "tÃºi tá»«" cá»§a TF-IDF Ä‘Ã´i khi láº¡i hiá»‡u quáº£ hÆ¡n phÆ°Æ¡ng phÃ¡p láº¥y trung bÃ¬nh vector má»™t cÃ¡ch ngÃ¢y thÆ¡.

#### **2. Dáº¥u Hiá»‡u Overfitting trong Tá»‘i Æ¯u HÃ³a cá»§a Optuna**
-   HÃ£y nhÃ¬n vÃ o cÃ¡c tham sá»‘ Optuna tÃ¬m Ä‘Æ°á»£c: `max_depth: 5` vÃ  `num_leaves: 146`.
-   ÄÃ¢y lÃ  má»™t **mÃ¢u thuáº«n lá»›n**. Má»™t cÃ¢y quyáº¿t Ä‘á»‹nh cÃ³ Ä‘á»™ sÃ¢u tá»‘i Ä‘a lÃ  5 (`max_depth=5`) chá»‰ cÃ³ thá»ƒ cÃ³ tá»‘i Ä‘a **2^5 = 32** lÃ¡ (`leaves`).
-   Viá»‡c Optuna chá»n `num_leaves=146` (nhiá»u hÆ¡n 32 ráº¥t nhiá»u) cho tháº¥y LightGBM Ä‘ang cá»‘ gáº¯ng táº¡o ra nhá»¯ng cÃ¢y ráº¥t "rá»™ng" vÃ  "nÃ´ng". NÃ³ Ä‘ang táº¡o ra ráº¥t nhiá»u quy táº¯c phÃ¢n chia ráº¥t cá»¥ thá»ƒ á»Ÿ cÃ¡c cáº¥p Ä‘á»™ tháº¥p mÃ  khÃ´ng xÃ¢y dá»±ng Ä‘Æ°á»£c cÃ¡c quy táº¯c tá»•ng quÃ¡t á»Ÿ cÃ¡c cáº¥p Ä‘á»™ cao hÆ¡n.
-   **NguyÃªn nhÃ¢n:** ÄÃ¢y lÃ  dáº¥u hiá»‡u kinh Ä‘iá»ƒn cá»§a viá»‡c mÃ´ hÃ¬nh Ä‘ang **overfit trÃªn táº­p validation** trong quÃ¡ trÃ¬nh tÃ¬m kiáº¿m cá»§a Optuna. NÃ³ Ä‘Ã£ tÃ¬m ra má»™t bá»™ tham sá»‘ "ká»³ láº¡" hoáº¡t Ä‘á»™ng tá»‘t trÃªn má»™t pháº§n nhá» dá»¯ liá»‡u Ä‘Ã³, nhÆ°ng láº¡i khÃ´ng cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t trÃªn táº­p test cuá»‘i cÃ¹ng.

## **4. HÆ°á»›ng Cáº£i Thiá»‡n cho Version 3.0 (Dá»±a trÃªn káº¿t quáº£ V2.2)**

ChÃºng ta Ä‘Ã£ há»c Ä‘Æ°á»£c ráº±ng: 1) khÃ´ng thá»ƒ bá» qua táº§m quan trá»ng cá»§a tá»« khÃ³a, vÃ  2) cáº§n kiá»ƒm soÃ¡t Optuna tá»‘t hÆ¡n. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c bÆ°á»›c Ä‘i tiáº¿p theo ráº¥t rÃµ rÃ ng.

### **Æ¯u tiÃªn #1: Káº¿t há»£p Sá»©c máº¡nh cá»§a TF-IDF vÃ  Word Embeddings (TF-IDF Weighted Embeddings)**
-   **Ã tÆ°á»Ÿng:** Thay vÃ¬ láº¥y trung bÃ¬nh cá»™ng cÃ¡c vector tá»«, chÃºng ta sáº½ láº¥y **trung bÃ¬nh cÃ³ trá»ng sá»‘**. Trá»ng sá»‘ cá»§a má»—i tá»« chÃ­nh lÃ  Ä‘iá»ƒm TF-IDF cá»§a tá»« Ä‘Ã³.
-   **Quy trÃ¬nh:**
    1.  Cháº¡y `TfidfVectorizer` nhÆ° V1.0 Ä‘á»ƒ cÃ³ Ä‘iá»ƒm sá»‘ cho tá»«ng tá»«.
    2.  Vá»›i má»—i abstract, khi táº¡o vector cuá»‘i cÃ¹ng, nhÃ¢n vector FastText cá»§a má»—i tá»« vá»›i Ä‘iá»ƒm TF-IDF cá»§a tá»« Ä‘Ã³, sau Ä‘Ã³ láº¥y tá»•ng vÃ  chia cho tá»•ng cÃ¡c Ä‘iá»ƒm TF-IDF.
-   **Lá»£i Ã­ch:** CÃ¡ch tiáº¿p cáº­n "lai" nÃ y giá»¯ láº¡i Ä‘Æ°á»£c **ngá»¯ nghÄ©a** cá»§a FastText vÃ  **táº§m quan trá»ng** cá»§a tá»« khÃ³a tá»« TF-IDF. CÃ¡c tá»« quan trá»ng sáº½ cÃ³ Ä‘Ã³ng gÃ³p lá»›n hÆ¡n vÃ o vector cuá»‘i cÃ¹ng.

### **Æ¯u tiÃªn #2: Tinh Chá»‰nh Láº¡i KhÃ´ng Gian TÃ¬m Kiáº¿m cá»§a Optuna**
-   **Váº¥n Ä‘á»:** Tham sá»‘ `num_leaves` vÃ  `max_depth` Ä‘ang mÃ¢u thuáº«n.
-   **Giáº£i phÃ¡p:** RÃ ng buá»™c khÃ´ng gian tÃ¬m kiáº¿m Ä‘á»ƒ nÃ³ há»£p lÃ½ hÆ¡n.
    -   Bá» `max_depth` ra khá»i danh sÃ¡ch tÃ¬m kiáº¿m vÃ  Ä‘áº·t má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh (vÃ­ dá»¥: -1 Ä‘á»ƒ khÃ´ng giá»›i háº¡n).
    -   Hoáº·c, rÃ ng buá»™c `num_leaves` trong hÃ m `objective`: `num_leaves = trial.suggest_int('num_leaves', 10, 2**params['max_depth'] - 1)`. Äiá»u nÃ y buá»™c sá»‘ lÃ¡ pháº£i nhá» hÆ¡n má»©c tá»‘i Ä‘a cho phÃ©p cá»§a Ä‘á»™ sÃ¢u.
-   **Gá»£i Ã½:** Báº¯t Ä‘áº§u báº±ng cÃ¡ch chá»‰ tá»‘i Æ°u `n_estimators`, `learning_rate`, `num_leaves`, `reg_alpha`, `reg_lambda`. ÄÃ¢y lÃ  nhá»¯ng tham sá»‘ cÃ³ tÃ¡c Ä‘á»™ng lá»›n nháº¥t.

### **Æ¯u tiÃªn #3 (Con Ä‘Æ°á»ng dÃ i háº¡n): Tiáº¿n tá»›i Contextual Embeddings**
-   Káº¿t quáº£ nÃ y cÃ ng cá»§ng cá»‘ thÃªm giáº£ thuyáº¿t ráº±ng cÃ¡c mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng hiá»ƒu **ngá»¯ cáº£nh** (nhÆ° SciBERT) sáº½ lÃ  chÃ¬a khÃ³a Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao nháº¥t, vÃ¬ chÃºng khÃ´ng cáº§n pháº£i láº¥y trung bÃ¬nh vector vÃ  cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c tá»« nÃ o lÃ  quan trá»ng trong má»™t cÃ¢u cá»¥ thá»ƒ.

## **5. Káº¿t Luáº­n Chung**

Version 2.2 lÃ  má»™t bÆ°á»›c tiáº¿n quan trá»ng. Máº·c dÃ¹ F1-score giáº£m, chÃºng ta Ä‘Ã£:
1.  **ThÃ nh cÃ´ng** giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u nÄƒng.
2.  **Há»c Ä‘Æ°á»£c ráº±ng** phÆ°Æ¡ng phÃ¡p láº¥y trung bÃ¬nh vector Ä‘Æ¡n giáº£n khÃ´ng Ä‘á»§ tá»‘t cho dá»¯ liá»‡u chuyÃªn ngÃ nh.
3.  **PhÃ¡t hiá»‡n ra** Ä‘iá»ƒm yáº¿u trong cÃ¡ch cáº¥u hÃ¬nh Optuna.

ÄÃ¢y lÃ  nhá»¯ng kinh nghiá»‡m quÃ½ bÃ¡u. Tháº¥t báº¡i trong viá»‡c cáº£i thiá»‡n metrics nhÆ°ng thÃ nh cÃ´ng trong viá»‡c thu tháº­p thÃ´ng tin Ä‘á»ƒ cÃ¡c phiÃªn báº£n sau tá»‘t hÆ¡n. Lá»™ trÃ¬nh cho V3.0 Ä‘Ã£ ráº¥t rÃµ rÃ ng: káº¿t há»£p TF-IDF vÃ  FastText, Ä‘á»“ng thá»i tinh chá»‰nh láº¡i quy trÃ¬nh tá»‘i Æ°u hÃ³a.

# Version 3
## Script
```python
# !pip install optuna
# !pip install gensim
# !python -m spacy download en_core_web_sm
# %pip install cuml-cu12 cudf-cu12

# ===================================================================
#      VERSION 3.2: GPU-ACCELERATED TF-IDF + WEIGHTED FASTTEXT
# ===================================================================
# Script nÃ y sá»­ dá»¥ng RAPIDS cuML Ä‘á»ƒ tÄƒng tá»‘c TF-IDF trÃªn GPU.
# PhiÃªn báº£n nÃ y Ä‘Ã£ sá»­a lá»—i TypeError khi xá»­ lÃ½ vocabulary cá»§a cuML.
# ===================================================================

# ===================================================================
# PHáº¦N 0: CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T
# ===================================================================
print("ğŸš€ Äang import cÃ¡c thÆ° viá»‡n...")
import pandas as pd
import numpy as np
import ast
import pickle
import os
import json
from collections import Counter
# ### V3.2 Cáº¢I TIáº¾N: ThÃªm thÆ° viá»‡n cá»§a RAPIDS ###
import cudf
from cuml.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
# CÃ¡c thÆ° viá»‡n cÃ²n láº¡i
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import f1_score, classification_report
from tqdm.auto import tqdm
import optuna
import gensim.downloader
import spacy
import warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)
print("âœ… Import thÆ° viá»‡n hoÃ n táº¥t.")

# ===================================================================
# PHáº¦N 0B: Cáº¤U HÃŒNH
# ===================================================================
N_TARGET_LABELS = 17
TFIDF_MAX_FEATURES = 15000

OPTUNA_N_TRIALS = 30
OPTUNA_TIMEOUT = 5400

LGBM_FIXED_PARAMS = {
    'device': 'gpu',
    'random_state': 42,
    'n_jobs': -1,
    'class_weight': 'balanced'
}
print("âš¡ï¸ ÄÃ£ kÃ­ch hoáº¡t cháº¿ Ä‘á»™ huáº¥n luyá»‡n GPU vÃ  cáº¥u hÃ¬nh cho Version 3.2 (GPU TF-IDF)!")

# ===================================================================
# PHáº¦N 1: Táº¢I Dá»® LIá»†U VÃ€ MÃ” HÃŒNH NLP
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 1/8] Táº£i dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh NLP...")
try:
    nlp = spacy.load("en_core_web_sm")
    print("   - MÃ´ hÃ¬nh spaCy 'en_core_web_sm' Ä‘Ã£ cÃ³ sáºµn.")
except OSError:
    print("   - Láº§n Ä‘áº§u cháº¡y, Ä‘ang cÃ i Ä‘áº·t vÃ  táº£i mÃ´ hÃ¬nh spaCy...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")
nlp.disable_pipes("parser", "ner")
print("   - Táº£i spaCy hoÃ n táº¥t.")

FILE_PATH = "/content/drive/MyDrive/AIO25/m04/data/arxiv_perfectly_balanced.csv"
try:
    df = pd.read_csv(FILE_PATH)
    print(f"âœ… Táº£i thÃ nh cÃ´ng file: '{FILE_PATH}' ({len(df):,} máº«u)")
except FileNotFoundError:
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{FILE_PATH}'.")
    exit()

df['parent_labels'] = df['parent_labels'].apply(ast.literal_eval)
df['child_labels'] = df['child_labels'].apply(ast.literal_eval)

# ===================================================================
# PHáº¦N 2: TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N VÃ€ HUáº¤N LUYá»†N TF-IDF TRÃŠN GPU
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 2/8] Tiá»n xá»­ lÃ½ vÄƒn báº£n vÃ  huáº¥n luyá»‡n TF-IDF trÃªn GPU...")

def preprocess_text_spacy(text):
    doc = nlp(str(text).lower())
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]
    return " ".join(tokens)

tqdm.pandas(desc="Tiá»n xá»­ lÃ½ Abstract")
df['processed_abstract'] = df['abstract'].progress_apply(preprocess_text_spacy)

print("   - Báº¯t Ä‘áº§u huáº¥n luyá»‡n TF-IDF trÃªn GPU (sáº½ nhanh hÆ¡n ráº¥t nhiá»u)...")
cudf_series = cudf.Series(df['processed_abstract'])
tfidf_vectorizer_gpu = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES)
tfidf_vectorizer_gpu.fit(cudf_series)

idf_values = tfidf_vectorizer_gpu.idf_
# ### Sá»¬A Lá»–I Láº¦N 2: DÃ¹ng phÆ°Æ¡ng phÃ¡p Ä‘áº£o ngÆ°á»£c Series, an toÃ n vÃ  hiá»‡u quáº£ ###
vocab_gpu = tfidf_vectorizer_gpu.vocabulary_
vocab_cpu = vocab_gpu.to_pandas()

# Táº¡o má»™t Series má»›i vá»›i index lÃ  chá»‰ sá»‘ cá»™t vÃ  value lÃ  tá»«, sau Ä‘Ã³ sáº¯p xáº¿p
index_to_word_series = pd.Series(vocab_cpu.index, index=vocab_cpu.values).sort_index()
# Láº¥y danh sÃ¡ch tá»« Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p chÃ­nh xÃ¡c
feature_names = index_to_word_series.to_list()

idf_weights = dict(zip(feature_names, idf_values))
print("âœ… Huáº¥n luyá»‡n TF-IDF trÃªn GPU vÃ  táº¡o trá»ng sá»‘ IDF thÃ nh cÃ´ng.")

# ===================================================================
# PHáº¦N 3: Táº¢I FASTTEXT VÃ€ Táº O VECTOR Káº¾T Há»¢P
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 3/8] Táº£i FastText vÃ  táº¡o vector Ä‘áº·c trÆ°ng káº¿t há»£p...")
print("   - Äang táº£i mÃ´ hÃ¬nh fasttext-wiki-news-subwords-300...")
ft_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')
embedding_dim = ft_model.vector_size
print(f"âœ… Táº£i mÃ´ hÃ¬nh FastText thÃ nh cÃ´ng (sá»‘ chiá»u: {embedding_dim}).")

def weighted_average_vector(processed_text, ft_model, idf_dict, dim):
    tokens = processed_text.split()
    weighted_vectors = []
    total_weight = 0.0
    for token in tokens:
        if token in ft_model.key_to_index and token in idf_dict:
            vector = ft_model[token]
            weight = idf_dict[token]
            weighted_vectors.append(vector * weight)
            total_weight += weight
    if not weighted_vectors: return np.zeros(dim)
    final_vector = np.sum(weighted_vectors, axis=0) / total_weight
    return final_vector

print("   - Äang táº¡o vector Ä‘áº·c trÆ°ng káº¿t há»£p cho cÃ¡c abstract...")
tqdm.pandas(desc="Táº¡o Vector Káº¿t Há»£p")
df['abstract_vector'] = df['processed_abstract'].progress_apply(
    lambda x: weighted_average_vector(x, ft_model, idf_weights, embedding_dim)
)

all_embeddings = np.vstack(df['abstract_vector'].values)
print(f"âœ… Táº¡o vector Ä‘áº·c trÆ°ng káº¿t há»£p hoÃ n táº¥t. KÃ­ch thÆ°á»›c: {all_embeddings.shape}")

# ===================================================================
# PHáº¦N 4: Tá»I Æ¯U HÃ“A SIÃŠU THAM Sá» Vá»šI OPTUNA
# ===================================================================
print(f"\nğŸš€ [BÆ°á»›c 4/8] Tá»‘i Æ°u siÃªu tham sá»‘ cho Táº§ng 1 vá»›i Optuna...")

parent_label_counts = Counter([item for sublist in df['parent_labels'] for item in sublist])
target_parents = [label for label, count in parent_label_counts.most_common(N_TARGET_LABELS)]
mlb_parent = MultiLabelBinarizer(classes=target_parents)
y_parent_binarized = mlb_parent.fit_transform(df['parent_labels'])
indices = df.index.values

X_train_emb, X_test_emb, y_train_p, y_test_p, indices_train, indices_test = train_test_split(
    all_embeddings, y_parent_binarized, indices, test_size=0.2, random_state=42
)

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 300, 1500, step=100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
    }
    params.update(LGBM_FIXED_PARAMS)
    
    X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(X_train_emb, y_train_p, test_size=0.25, random_state=42)
    
    model = OneVsRestClassifier(LGBMClassifier(**params), n_jobs=1)
    model.fit(X_train_opt, y_train_opt)
    preds = model.predict(X_val_opt)
    score = f1_score(y_val_opt, preds, average='weighted', zero_division=0)
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=OPTUNA_N_TRIALS, timeout=OPTUNA_TIMEOUT)
best_params_tier1 = study.best_params
print(f"âœ… Tá»‘i Æ°u hÃ³a hoÃ n táº¥t sau {len(study.trials)} trials.")
print(f"   - F1-Score tá»‘t nháº¥t trÃªn táº­p validation: {study.best_value:.4f}")
print(f"   - SiÃªu tham sá»‘ tá»‘i Æ°u: {best_params_tier1}")

# ===================================================================
# PHáº¦N 5: HUáº¤N LUYá»†N Táº¦NG 1
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 5/8] Huáº¥n luyá»‡n Táº§ng 1 vá»›i tham sá»‘ tá»‘t nháº¥t...")
final_params = best_params_tier1.copy()
final_params.update(LGBM_FIXED_PARAMS)
parent_model = OneVsRestClassifier(LGBMClassifier(**final_params), n_jobs=1)
parent_model.fit(X_train_emb, y_train_p)
print("âœ… Huáº¥n luyá»‡n Táº§ng 1 hoÃ n táº¥t.")

# ===================================================================
# PHáº¦N 6: HUáº¤N LUYá»†N Táº¦NG 2
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 6/8] Huáº¥n luyá»‡n Táº§ng 2...")
tier2_classifiers, tier2_mlbs = {}, {}
df_train = df.loc[indices_train]
for parent_label in tqdm(mlb_parent.classes_, desc="Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Táº§ng 2"):
    indices_with_parent_local = [i for i, labels in enumerate(df_train['parent_labels']) if parent_label in labels]
    if len(indices_with_parent_local) < 20: continue
    df_child, X_child_emb = df_train.iloc[indices_with_parent_local], X_train_emb[indices_with_parent_local]
    y_child_raw = df_child['child_labels'].apply(lambda l: [c for c in l if c.startswith(parent_label)])
    if y_child_raw.apply(len).sum() == 0: continue
    mlb_child = MultiLabelBinarizer()
    y_child_binarized = mlb_child.fit_transform(y_child_raw)
    if y_child_binarized.shape[1] < 2: continue
    child_model = OneVsRestClassifier(LGBMClassifier(**final_params), n_jobs=1)
    child_model.fit(X_child_emb, y_child_binarized)
    tier2_classifiers[parent_label], tier2_mlbs[parent_label] = child_model, mlb_child
print(f"\nâœ… ÄÃ£ huáº¥n luyá»‡n {len(tier2_classifiers)} mÃ´ hÃ¬nh Táº§ng 2.")

# ===================================================================
# PHáº¦N 7: ÄÃNH GIÃ
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 7/8] ÄÃ¡nh giÃ¡ vÃ  táº¡o bÃ¡o cÃ¡o metrics chi tiáº¿t...")
df_test = df.loc[indices_test]
emb_test = X_test_emb

true_child_labels_raw = df_test['child_labels'].tolist()
mlb_all_children = MultiLabelBinarizer().fit(df['child_labels'])
y_test_child_true_binarized = mlb_all_children.transform(true_child_labels_raw)
y_pred_parent_binarized = parent_model.predict(emb_test)
final_parents_raw = mlb_parent.inverse_transform(y_pred_parent_binarized)
final_predictions_raw = []
for i in tqdm(range(len(df_test)), desc="Dá»± Ä‘oÃ¡n Táº§ng 2 trÃªn táº­p test"):
    predicted_parents = final_parents_raw[i]
    child_preds = set()
    if predicted_parents:
        emb_vector = emb_test[i:i+1]
        for parent in predicted_parents:
            if parent in tier2_classifiers:
                child_model, child_mlb = tier2_classifiers[parent], tier2_mlbs[parent]
                pred_child_binarized = child_model.predict(emb_vector)
                child_preds.update(child_mlb.inverse_transform(pred_child_binarized)[0])
    final_predictions_raw.append(sorted(list(child_preds)))
y_pred_child_final_binarized = mlb_all_children.transform(final_predictions_raw)
metrics_report = {}
report_parent_dict = classification_report(y_test_p, y_pred_parent_binarized, target_names=mlb_parent.classes_, output_dict=True, zero_division=0)
metrics_report['f1_macro_parent'] = report_parent_dict['macro avg']['f1-score']
metrics_report['f1_weighted_parent'] = report_parent_dict['weighted avg']['f1-score']
metrics_report['f1_samples_parent'] = f1_score(y_test_p, y_pred_parent_binarized, average='samples', zero_division=0)
metrics_report['f1_macro_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='macro', zero_division=0)
metrics_report['f1_weighted_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='weighted', zero_division=0)
metrics_report['f1_samples_children_overall'] = f1_score(y_test_child_true_binarized, y_pred_child_final_binarized, average='samples', zero_division=0)
metrics_report['best_hyperparameters_tier1'] = study.best_params

print("\n" + "="*80)
print(" " * 12 + "BÃO CÃO HIá»†U SUáº¤T Há»† THá»NG - VERSION 3.2 (GPU TF-IDF)")
print("="*80)
print(f"\n   - SIÃŠU THAM Sá» Tá»I Æ¯U (tá»« Optuna):")
for key, value in metrics_report['best_hyperparameters_tier1'].items():
    if isinstance(value, float): print(f"     - {key}: {value:.4f}")
    else: print(f"     - {key}: {value}")
print("\n--- Táº§ng 1 (Dá»± Ä‘oÃ¡n 17 NhÃ£n Cha chÃ­nh) ---")
print(f"   - â­ï¸ F1-Score (Weighted Avg): {metrics_report['f1_weighted_parent']:.4f}")
print(f"   - F1-Score (Macro Avg):        {metrics_report['f1_macro_parent']:.4f}")
print("\n--- ToÃ n Há»‡ Thá»‘ng (Dá»± Ä‘oÃ¡n NhÃ£n Con Cuá»‘i CÃ¹ng) ---")
print(f"   - â­ï¸ F1-Score (Weighted Avg): {metrics_report['f1_weighted_children_overall']:.4f}")
print(f"   - F1-Score (Macro Avg):        {metrics_report['f1_macro_children_overall']:.4f}")
print("\n" + "="*80)

# ===================================================================
# PHáº¦N 8: LÆ¯U Káº¾T QUáº¢
# ===================================================================
print("\nğŸš€ [BÆ°á»›c 8/8] LÆ°u káº¿t quáº£ vÃ  cÃ¡c thÃ nh pháº§n...")
MODEL_DIR = "/content/drive/MyDrive/AIO25/m04/data/saved_models_v3.2_gpu_tfidf_weighted_fasttext/"
os.makedirs(MODEL_DIR, exist_ok=True)
with open(os.path.join(MODEL_DIR, 'tier1_classifier.pkl'), 'wb') as f: pickle.dump(parent_model, f)
with open(os.path.join(MODEL_DIR, 'tier2_classifiers.pkl'), 'wb') as f: pickle.dump(tier2_classifiers, f)
with open(os.path.join(MODEL_DIR, 'tier1_mlb.pkl'), 'wb') as f: pickle.dump(mlb_parent, f)
with open(os.path.join(MODEL_DIR, 'tier2_mlbs.pkl'), 'wb') as f: pickle.dump(tier2_mlbs, f)
with open(os.path.join(MODEL_DIR, 'cuml_tfidf_vectorizer_v3.pkl'), 'wb') as f: pickle.dump(tfidf_vectorizer_gpu, f)
with open(os.path.join(MODEL_DIR, 'metrics_report.json'), 'w') as f: json.dump(metrics_report, f, indent=4)
print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng cÃ¡c thÃ nh pháº§n mÃ´ hÃ¬nh vÃ o: {MODEL_DIR}")
```
**Version:** 3.2 - GPU TF-IDF Weighted FastText + Smart Optuna  
**So vá»›i cÃ¡c phiÃªn báº£n trÆ°á»›c:** Má»™t thá»­ nghiá»‡m káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng nháº±m táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.

## **1. Má»¥c TiÃªu**

Version 3.2 Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nhá»¯ng bÃ i há»c tá»« V1.0 vÃ  V2.2, vá»›i má»™t má»¥c tiÃªu Ä‘áº§y tham vá»ng:
1.  **Káº¿t há»£p "Tá»‘t nháº¥t cá»§a cáº£ hai tháº¿ giá»›i":** Táº¡o ra má»™t vector Ä‘áº·c trÆ°ng duy nháº¥t vá»«a cÃ³ kháº£ nÄƒng **hiá»ƒu ngá»¯ nghÄ©a** (tá»« FastText) vá»«a **nháº¥n máº¡nh táº§m quan trá»ng cá»§a cÃ¡c tá»« khÃ³a hiáº¿m** (tá»« TF-IDF).
2.  **Duy trÃ¬ hiá»‡u nÄƒng cao:** Tiáº¿p tá»¥c sá»­ dá»¥ng RAPIDS cuML Ä‘á»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh tÃ­nh toÃ¡n TF-IDF trÃªn GPU.
3.  **Tá»‘i Æ°u hÃ³a thÃ´ng minh:** Ãp dá»¥ng quy trÃ¬nh tinh chá»‰nh siÃªu tham sá»‘ báº±ng Optuna trÃªn bá»™ Ä‘áº·c trÆ°ng "lai" má»›i nÃ y.

## **2. Kiáº¿n TrÃºc & PhÆ°Æ¡ng PhÃ¡p Thá»±c Hiá»‡n (Chi tiáº¿t)**

ÄÃ¢y lÃ  pipeline hoÃ n chá»‰nh cá»§a Version 3.2, má»™t kiáº¿n trÃºc phá»©c táº¡p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phiÃªn báº£n trÆ°á»›c.

1.  **Tiá»n xá»­ lÃ½ vÄƒn báº£n (vá»›i spaCy):**
    -   Má»—i abstract Ä‘Æ°á»£c Ä‘Æ°a qua má»™t pipeline tiá»n xá»­ lÃ½: chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng, loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, vÃ  quan trá»ng nháº¥t lÃ  **lemmatization** (Ä‘Æ°a tá»« vá» dáº¡ng gá»‘c, vÃ­ dá»¥: `studies`, `studying` -> `study`).
    -   Káº¿t quáº£ lÃ  má»™t phiÃªn báº£n "sáº¡ch" cá»§a abstract, sáºµn sÃ ng cho cÃ¡c bÆ°á»›c tiáº¿p theo.

2.  **Huáº¥n luyá»‡n TF-IDF trÃªn GPU (Chá»‰ Ä‘á»ƒ láº¥y trá»ng sá»‘):**
    -   ToÃ n bá»™ 30,000 abstract Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Æ°á»£c Ä‘Æ°a vÃ o `TfidfVectorizer` cá»§a `cuML`.
    -   MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c `fit` trÃªn dá»¯ liá»‡u Ä‘á»ƒ há»c vÃ  tÃ­nh toÃ¡n **trá»ng sá»‘ IDF (Inverse Document Frequency)** cho 15,000 tá»« phá»• biáº¿n nháº¥t. IDF lÃ  má»™t thÆ°á»›c Ä‘o cho biáº¿t má»™t tá»« hiáº¿m hay phá»• biáº¿n trong toÃ n bá»™ kho vÄƒn báº£n.
    -   **LÆ°u Ã½ quan trá»ng:** ChÃºng ta **khÃ´ng** sá»­ dá»¥ng ma tráº­n TF-IDF mÃ  nÃ³ táº¡o ra. Má»¥c Ä‘Ã­ch duy nháº¥t cá»§a bÆ°á»›c nÃ y lÃ  Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t dictionary `idf_weights` chá»©a Ä‘iá»ƒm sá»‘ hiáº¿m cá»§a tá»«ng tá»«.

3.  **Táº£i mÃ´ hÃ¬nh FastText:**
    -   MÃ´ hÃ¬nh `fasttext-wiki-news-subwords-300` Ä‘Æ°á»£c táº£i vá». MÃ´ hÃ¬nh nÃ y chá»©a cÃ¡c vector 300 chiá»u Ä‘áº¡i diá»‡n cho ngá»¯ nghÄ©a cá»§a hÃ ng triá»‡u tá»«.

4.  **Táº¡o Vector Äáº·c TrÆ°ng "Lai" (Cá»‘t lÃµi cá»§a V3.2):**
    -   ÄÃ¢y lÃ  bÆ°á»›c Ä‘á»™t phÃ¡ vÃ  cÅ©ng lÃ  trung tÃ¢m cá»§a thá»­ nghiá»‡m. Vá»›i má»—i abstract, chÃºng tÃ´i thá»±c hiá»‡n:
        a. TÃ¡ch abstract thÃ nh cÃ¡c token (tá»«).
        b. Vá»›i má»—i token, láº¥y ra **vector FastText** (300 chiá»u) vÃ  **trá»ng sá»‘ IDF** cá»§a nÃ³.
        c. NhÃ¢n vector FastText vá»›i trá»ng sá»‘ IDF. Thao tÃ¡c nÃ y khuáº¿ch Ä‘áº¡i Ä‘á»™ lá»›n (magnitude) cá»§a vector Ä‘á»‘i vá»›i cÃ¡c tá»« hiáº¿m vÃ  giáº£m Ä‘á»™ lá»›n Ä‘á»‘i vá»›i cÃ¡c tá»« phá»• biáº¿n.
        d. TÃ­nh **trung bÃ¬nh cÃ³ trá»ng sá»‘** cá»§a táº¥t cáº£ cÃ¡c vector Ä‘Ã£ Ä‘Æ°á»£c khuáº¿ch Ä‘áº¡i nÃ y Ä‘á»ƒ táº¡o ra má»™t vector 300 chiá»u duy nháº¥t Ä‘áº¡i diá»‡n cho toÃ n bá»™ abstract.
    -   **Ká»³ vá»ng:** Vector cuá»‘i cÃ¹ng sáº½ vá»«a mang thÃ´ng tin ngá»¯ nghÄ©a, vá»«a Ä‘Æ°á»£c "lÃ¡i" theo hÆ°á»›ng cá»§a cÃ¡c tá»« khÃ³a quan trá»ng nháº¥t.

5.  **Tá»‘i Æ°u hÃ³a vÃ  Huáº¥n luyá»‡n:**
    -   Vector 300 chiá»u má»›i nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘áº§u vÃ o cho quy trÃ¬nh Optuna vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n cáº¥p hai táº§ng LightGBM, tÆ°Æ¡ng tá»± nhÆ° cÃ¡c phiÃªn báº£n trÆ°á»›c.

## **3. Káº¿t Quáº£ Thá»­ Nghiá»‡m (Version 3.2)**

### **Báº£ng So SÃ¡nh Hiá»‡u Suáº¥t: V1.0 vs V2.2 vs V3.2**

| Metric | V1.0 (TF-IDF 5k) | V2.2 (FastText Avg) | **V3.2 (TF-IDF Weighted)** | PhÃ¢n TÃ­ch |
| :--- | :--- | :--- | :--- | :--- |
| **F1-Weighted (Táº§ng 1)** | **0.6483** | 0.6386 | **0.0951** | **â†“ 85%** (Sá»¥p Ä‘á»•) |
| **F1-Macro (Táº§ng 1)** | **0.6474** | 0.6359 | **0.0835** | **â†“ 87%** (Sá»¥p Ä‘á»•) |
| **F1-Weighted (Táº§ng 2)** | **0.4047** | 0.3732 | **0.0264** | **â†“ 93%** (Tháº¥t báº¡i hoÃ n toÃ n) |
| **F1-Macro (Táº§ng 2)** | **0.2543** | 0.2319 | **0.0080** | **â†“ 97%** (Tháº¥t báº¡i hoÃ n toÃ n) |

## **4. PhÃ¢n TÃ­ch ChuyÃªn SÃ¢u: Táº¡i Sao Káº¿t Quáº£ Láº¡i Tá»‡ Háº¡i NhÆ° Váº­y?**

Káº¿t quáº£ khÃ´ng chá»‰ khÃ´ng cáº£i thiá»‡n mÃ  cÃ²n sá»¥p Ä‘á»• hoÃ n toÃ n. ÄÃ¢y khÃ´ng pháº£i lÃ  má»™t sá»± sá»¥t giáº£m thÃ´ng thÆ°á»ng mÃ  lÃ  dáº¥u hiá»‡u cá»§a má»™t **sai láº§m cÆ¡ báº£n trong phÆ°Æ¡ng phÃ¡p luáº­n** khi káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng.

**NguyÃªn nhÃ¢n chÃ­nh: Sá»± Thá»‘ng Trá»‹ cá»§a cÃ¡c Tá»« SiÃªu Hiáº¿m vÃ  Sá»± "Nhiá»…u Loáº¡n" Ngá»¯ NghÄ©a**

1.  **Khuáº¿ch Äáº¡i TÃ­n Hiá»‡u QuÃ¡ Má»©c:** Trá»ng sá»‘ IDF cÃ³ thang Ä‘o logarit. Má»™t tá»« xuáº¥t hiá»‡n trong 10 tÃ i liá»‡u sáº½ cÃ³ Ä‘iá»ƒm IDF cao hÆ¡n ráº¥t nhiá»u so vá»›i má»™t tá»« xuáº¥t hiá»‡n trong 1,000 tÃ i liá»‡u. Khi chÃºng ta nhÃ¢n vector FastText (cÃ³ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i Ä‘á»“ng Ä‘á»u) vá»›i Ä‘iá»ƒm IDF nÃ y, vector cá»§a cÃ¡c tá»« **siÃªu hiáº¿m** (vÃ­ dá»¥: má»™t thuáº­t ngá»¯ ráº¥t háº¹p, má»™t lá»—i chÃ­nh táº£,...) sáº½ bá»‹ khuáº¿ch Ä‘áº¡i lÃªn gáº¥p 10, 20 láº§n so vá»›i cÃ¡c tá»« khÃ¡c.

2.  **"Pha LoÃ£ng" vÃ  "BÃ³p MÃ©o" Ngá»¯ NghÄ©a:**
    -   HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t abstract vá» "Computer Science" cÃ³ cÃ¡c tá»«: `learning` (phá»• biáº¿n, IDF tháº¥p), `network` (phá»• biáº¿n, IDF tháº¥p), vÃ  má»™t thuáº­t ngá»¯ toÃ¡n há»c ráº¥t hiáº¿m `Grothendieck-Riemann-Roch` (siÃªu hiáº¿m, IDF cá»±c cao).
    -   Trong phÆ°Æ¡ng phÃ¡p **Vector Trung BÃ¬nh (V2.2)**, `Grothendieck...` chá»‰ Ä‘Ã³ng gÃ³p má»™t pháº§n nhá».
    -   Trong phÆ°Æ¡ng phÃ¡p **TF-IDF Weighted (V3.2)**, vector cá»§a `Grothendieck...` sáº½ Ä‘Æ°á»£c nhÃ¢n vá»›i má»™t sá»‘ ráº¥t lá»›n. Vector 300 chiá»u cuá»‘i cÃ¹ng sáº½ gáº§n nhÆ° chá»‰ lÃ  vector cá»§a `Grothendieck...` vÃ  bá»‹ bÃ³p mÃ©o hoÃ n toÃ n. NÃ³ Ä‘Ã£ **máº¥t háº¿t thÃ´ng tin ngá»¯ nghÄ©a** cá»§a `learning` vÃ  `network`.
    -   MÃ´ hÃ¬nh khÃ´ng cÃ²n há»c vá» "Khoa há»c MÃ¡y tÃ­nh" ná»¯a, mÃ  nÃ³ Ä‘ang cá»‘ gáº¯ng phÃ¢n loáº¡i dá»±a trÃªn nhá»¯ng thuáº­t ngá»¯ dá»‹ biá»‡t, nhiá»…u vÃ  khÃ´ng mang tÃ­nh Ä‘áº¡i diá»‡n cho chá»§ Ä‘á» chÃ­nh.

3.  **So sÃ¡nh vá»›i TF-IDF Thuáº§n TÃºy (V1.0):**
    -   Trong V1.0, `learning`, `network`, vÃ  `Grothendieck` lÃ  3 cá»™t (feature) riÃªng biá»‡t trong ma tráº­n 15,000 chiá»u. MÃ´ hÃ¬nh LightGBM Ä‘á»§ thÃ´ng minh Ä‘á»ƒ há»c ráº±ng `learning` vÃ  `network` lÃ  nhá»¯ng tÃ­n hiá»‡u máº¡nh cho lá»›p `cs`, trong khi `Grothendieck` cÃ³ thá»ƒ lÃ  má»™t tÃ­n hiá»‡u nhiá»…u hoáº·c chá»‰ quan trá»ng trong má»™t sá»‘ trÆ°á»ng há»£p ráº¥t háº¹p.
    -   Trong V3.2, chÃºng ta Ä‘Ã£ **Ã©p** cáº£ ba tÃ­n hiá»‡u nÃ y vÃ o má»™t vector 300 chiá»u duy nháº¥t má»™t cÃ¡ch sai láº§m, khiáº¿n tÃ­n hiá»‡u nhiá»…u láº¥n Ã¡t hoÃ n toÃ n tÃ­n hiá»‡u chÃ­nh **trÆ°á»›c khi** mÃ´ hÃ¬nh cÃ³ cÆ¡ há»™i há»c.

## **5. BÃ i Há»c RÃºt Ra vÃ  HÆ°á»›ng Äi Tiáº¿p Theo**

Tháº¥t báº¡i cá»§a V3.2 lÃ  bÃ i há»c quÃ½ giÃ¡ nháº¥t tá»« trÆ°á»›c Ä‘áº¿n nay.
-   **BÃ i há»c:** Viá»‡c káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng má»™t cÃ¡ch "ngÃ¢y thÆ¡" cÃ³ thá»ƒ phÃ¡ há»§y thÃ´ng tin thay vÃ¬ lÃ m giÃ u nÃ³. Pháº£i luÃ´n hiá»ƒu rÃµ báº£n cháº¥t vÃ  thang Ä‘o cá»§a tá»«ng loáº¡i Ä‘áº·c trÆ°ng trÆ°á»›c khi káº¿t há»£p.
-   **XÃ¡c nháº­n:** TF-IDF váº«n lÃ  má»™t baseline cá»±c ká»³ máº¡nh máº½ cho cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i vÄƒn báº£n dá»±a trÃªn tá»« khÃ³a.

**HÆ°á»›ng Ä‘i cho Version 4.0: Giá»¯ Láº¡i ThÃ´ng Tin Thay VÃ¬ PhÃ¡ Há»§y NÃ³**

ChÃºng ta sáº½ khÃ´ng cá»‘ gáº¯ng "Ã©p" cÃ¡c loáº¡i Ä‘áº·c trÆ°ng vÃ o cÃ¹ng má»™t khÃ´ng gian ná»¯a. Thay vÃ o Ä‘Ã³, chÃºng ta sáº½ cho mÃ´ hÃ¬nh tháº¥y táº¥t cáº£ chÃºng.

-   **PhÆ°Æ¡ng phÃ¡p:** **Ná»‘i Äáº·c TrÆ°ng (Feature Concatenation)**
    1.  Táº¡o ma tráº­n TF-IDF 15,000 chiá»u tá»« V1.0 (sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c).
    2.  Táº¡o ma tráº­n FastText 300 chiá»u tá»« V2.2 (dÃ¹ng vector trung bÃ¬nh Ä‘Æ¡n giáº£n).
    3.  **Ná»‘i (concatenate)** hai ma tráº­n nÃ y láº¡i vá»›i nhau theo chiá»u ngang Ä‘á»ƒ táº¡o ra má»™t ma tráº­n Ä‘áº·c trÆ°ng cuá»‘i cÃ¹ng cÃ³ `15,000 + 300 = 15,300` chiá»u cho má»—i abstract.
-   **Lá»£i Ã­ch:**
    -   **Báº£o toÃ n thÃ´ng tin:** MÃ´ hÃ¬nh sáº½ nháº­n Ä‘Æ°á»£c cáº£ hai dáº¡ng thÃ´ng tin má»™t cÃ¡ch riÃªng biá»‡t: 15,000 cá»™t cho tÃ­n hiá»‡u tá»« khÃ³a vÃ  300 cá»™t cho tÃ­n hiá»‡u ngá»¯ nghÄ©a.
    -   **Táº­n dá»¥ng sá»©c máº¡nh cá»§a LightGBM:** LightGBM vÃ  cÃ¡c mÃ´ hÃ¬nh cÃ¢y quyáº¿t Ä‘á»‹nh khÃ¡c cá»±c ká»³ giá»i trong viá»‡c xá»­ lÃ½ cÃ¡c khÃ´ng gian Ä‘áº·c trÆ°ng cÃ³ sá»‘ chiá»u lá»›n vÃ  tá»± Ä‘á»™ng chá»n ra nhá»¯ng Ä‘áº·c trÆ°ng quan trá»ng nháº¥t Ä‘á»ƒ phÃ¢n loáº¡i.

## **6. Káº¿t Luáº­n Chung**

Version 3.2 lÃ  má»™t tháº¥t báº¡i vá» máº·t metrics nhÆ°ng lÃ  má»™t thÃ nh cÃ´ng lá»›n vá» máº·t khoa há»c. NÃ³ Ä‘Ã£ chá»‰ ra má»™t cÃ¡ch rÃµ rÃ ng ráº±ng phÆ°Æ¡ng phÃ¡p lai "TF-IDF Weighted Embedding" lÃ  khÃ´ng phÃ¹ há»£p cho bÃ i toÃ¡n nÃ y. Káº¿t quáº£ nÃ y giÃºp chÃºng ta loáº¡i bá» má»™t hÆ°á»›ng Ä‘i sai láº§m vÃ  cá»§ng cá»‘ cho má»™t hÆ°á»›ng Ä‘i má»›i, há»©a háº¹n hÆ¡n cho V4.0: ná»‘i Ä‘áº·c trÆ°ng.

